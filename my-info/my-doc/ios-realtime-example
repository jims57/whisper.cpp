For iOS real-time audio transcription using whisper.cpp, you'll need a different approach than the SDL2 example. Here's a conceptual example of how you would implement this in iOS:

import AVFoundation
import AudioToolbox

class AudioTranscriptionViewController: UIViewController {
    private var audioEngine: AVAudioEngine!
    private var inputNode: AVAudioInputNode!
    private var whisperContext: WhisperContext? // Bridge to whisper.cpp
    
    // Buffer configuration similar to the stream example
    private let sampleRate: Double = 16000
    private let stepMs: Int = 500   // Sample every 500ms like the example
    private let lengthMs: Int = 5000 // 5000ms context
    
    override func viewDidLoad() {
        super.viewDidLoad()
        setupAudio()
        setupWhisper()
    }
    
    private func setupWhisper() {
        // Initialize whisper context
        guard let modelPath = Bundle.main.path(forResource: "ggml-base.en", ofType: "bin") else {
            print("Model not found")
            return
        }
        whisperContext = WhisperContext(modelPath: modelPath)
    }
    
    private func setupAudio() {
        audioEngine = AVAudioEngine()
        inputNode = audioEngine.inputNode
        
        let recordingFormat = AVAudioFormat(commonFormat: .pcmFormatFloat32,
                                          sampleRate: sampleRate,
                                          channels: 1,
                                          interleaved: false)!
        
        inputNode.installTap(onBus: 0, bufferSize: AVAudioFrameCount(sampleRate * 0.5), // 500ms buffer
                           format: recordingFormat) { [weak self] buffer, time in
            // Process audio buffer
            self?.processAudio(buffer)
        }
        
        do {
            try audioEngine.start()
        } catch {
            print("Could not start audio engine: \(error)")
        }
    }
    
    private func processAudio(_ buffer: AVAudioPCMBuffer) {
        // Convert buffer to format whisper.cpp expects
        guard let whisperContext = whisperContext,
              let channelData = buffer.floatChannelData?[0] else { return }
        
        // Process audio with whisper.cpp
        whisperContext.processAudio(channelData, sampleCount: Int(buffer.frameLength)) { result in
            // Handle transcription result
            DispatchQueue.main.async {
                print("Transcription: \(result)")
            }
        }
    }
}

#import <Foundation/Foundation.h>

@interface WhisperContext : NSObject

- (instancetype)initWithModelPath:(NSString *)path;
- (void)processAudio:(float *)samples sampleCount:(int)sampleCount completion:(void(^)(NSString *result))completion;

@end

#import "WhisperContext.h"
#import "whisper.h"

@implementation WhisperContext {
    struct whisper_context * ctx;
}

- (instancetype)initWithModelPath:(NSString *)path {
    self = [super init];
    if (self) {
        ctx = whisper_init_from_file(path.UTF8String);
    }
    return self;
}

- (void)processAudio:(float *)samples sampleCount:(int)sampleCount completion:(void(^)(NSString *))completion {
    // Create whisper parameters similar to stream example
    whisper_full_params params = whisper_full_default_params(WHISPER_SAMPLING_GREEDY);
    params.print_realtime = true;
    params.print_progress = false;
    params.single_segment = true;
    params.max_tokens = 32;
    params.translate = false;
    params.language = "en";
    params.n_threads = 8;

    // Process audio
    if (whisper_full(ctx, params, samples, sampleCount) == 0) {
        // Get the transcription result
        const int n_segments = whisper_full_n_segments(ctx);
        NSMutableString *result = [NSMutableString string];
        
        for (int i = 0; i < n_segments; ++i) {
            const char* text = whisper_full_get_segment_text(ctx, i);
            [result appendString:@(text)];
        }
        
        completion(result);
    }
}

- (void)dealloc {
    if (ctx) {
        whisper_free(ctx);
    }
}

@end

Key differences from the SDL2 example:
Uses AVAudioEngine instead of SDL2 for audio capture
Implements native iOS audio handling
Uses Objective-C++ bridge to interface with whisper.cpp
Handles audio buffers in a streaming fashion similar to the original example
To use this:
Add the whisper.cpp source files to your Xcode project
2. Include the model file in your app bundle
Set up proper bridging headers
Configure your audio permissions in Info.plist:
<key>NSMicrophoneUsageDescription</key>
<string>We need access to your microphone for real-time transcription.</string>

Remember to:
Request microphone permissions at runtime
Handle audio session interruptions
Consider background audio handling if needed
Implement proper error handling
Consider memory management for long-running transcriptions
This implementation maintains the same basic concept as the original example (sampling every 500ms with a 5000ms context) but adapts it to iOS's native audio framework.